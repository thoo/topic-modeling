{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T20:19:44.561664Z",
     "start_time": "2018-10-10T20:19:43.751588Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T20:19:44.735741Z",
     "start_time": "2018-10-10T20:19:44.564271Z"
    }
   },
   "outputs": [],
   "source": [
    "from static_grader import grader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Analyzing Review Text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unstructured data makes up the vast majority of data.  This is a basic intro to handling unstructured data.  Our objective is to be able to extract the sentiment (positive or negative) from review text.  We will do this from Yelp review data.\n",
    "\n",
    "The first three questions task you to build models, of increasing complexity, to predict the rating of a review from its text.  These models will be assessed based on the root mean squared error of the number of stars predicted.  There is a reference solution (which should not be too hard to beat) that defines the score of 1.\n",
    "\n",
    "The final question asks only for the result of a calculation, and your results will be compared directly to those of a reference solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It **is** possible to score >1 on these questions. This indicates that you've beaten our reference model - we compare our model's score on a test set to your score on a test set. See how high you can go!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and parse the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, let's download the data set from Amazon S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T03:58:32.308409Z",
     "start_time": "2018-10-09T03:58:30.109946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dataincubator-course/mldata/yelp_train_academic_dataset_review_reduced.json.gz to ./yelp_train_academic_dataset_review_reduced.json.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' --include 'yelp_train_academic_dataset_review_reduced.json.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data are a series of JSON objects, in a Gzipped file. Python supports Gzipped files natively: [`gzip.open`](https://docs.python.org/2/library/gzip.html) has the same interface as `open`, but handles `.gz` files automatically.\n",
    "\n",
    "The built-in json package has a `loads()` function that converts a JSON string into a Python dictionary.  We could call that once for each row of the file. [`ujson`](http://docs.micropython.org/en/latest/library/ujson.html) has the same interface as the built-in `json` library, but is *substantially* faster (at the cost of non-robust handling of malformed json).  We will use that inside a list comprehension to get a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T13:32:20.073207Z",
     "start_time": "2018-10-09T13:32:15.538369Z"
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import ujson as json\n",
    "\n",
    "with gzip.open('yelp_train_academic_dataset_review_reduced.json.gz') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit Learn will want the labels in a separate data structure, so let's pull those out now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data=pd.read_json('yelp_train_academic_dataset_review_reduced.json.gz',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = [row['stars'] for row in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Pandas](http://pandas.pydata.org/) is able to read JSON text directly.  Use the `read_json()` function with the `lines=True` keyword argument.  While the rest of this notebook will assume you are using a list of dictionaries, you can complete it with dataframes, if you so desire. Some of the example code will need to be modified in this case.\n",
    "\n",
    "2. There are obvious mistakes in the data.  There is no need to try to correct them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first three questions, you will need to build and train an estimator to predict the star rating from the text of a review.  We recommend building a pipeline out of transformers and estimators provided by Scikit Learn.  You can decide whether these pipelines should take full review objects or just their text as input to the `fit()` and `predict()` methods, but it does pay to be consistent.\n",
    "\n",
    "You may find it useful to serialize the trained models to disk.  This will allow you to reload the models after restarting the notebook, without needing to retrain them.  We recommend using the [`dill` library](https://pypi.python.org/pypi/dill) for this (although the [`joblib` library](http://scikit-learn.org/stable/modules/model_persistence.html) also works).  Use\n",
    "```python\n",
    "dill.dump(estimator, open('estimator.dill', 'w'))\n",
    "```\n",
    "to serialize the object `estimator` to the file `estimator.dill`.  If you have trouble with this, try setting the `recurse=True` keyword arguments in the call of `dill.dump()`.  The estimator can be deserialized with\n",
    "```python\n",
    "estimator = dill.load(open('estimator.dill', 'r'))\n",
    "```\n",
    "\n",
    "You may run into trouble with the size of your models and Digital Ocean's memory limit. This is a major concern in real-world applications. Your production environment will likely not be that different from Digital Ocean and being able to deploy there is important. Think about what information the different stages of\n",
    "your pipeline need and how you can reduce the memory footprint.\n",
    "\n",
    "Additionally, you may notice that your serialized models are very large and take a long time to load.  Some hints to reduce their size:\n",
    "\n",
    "- If you are using `GridSearchCV` to find the optimal values of hyperparameters (and you should be), the resultant object will contain many copies of the estimator that aren't needed any more.  Instead of serializing the whole `GridSearchCV`, serialize just the estimator with the correct hyperparameters.  This can be accessed through the `.best_estimator_` attribute of the `GridSearchCV` object.  Alternatively, the `.best_params_` attribute gives the best values of the hyperparameters.\n",
    "\n",
    "- The `CountVectorizer` keeps track of all words that were excluded from vectorization in its `.stop_words_` attribute.  This can be interesting to examine, but isn't needed for predictions.  Set this attribute to the empty list before serializing it to save disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the \"model\" questions asks you to create a function that models the number of stars given in a review from the review text.  It will be passed a list of dictionaries.  Each of these will have the same format as the JSON objects you've just read in.  This function should return a list of numbers of the same length, giving the predicted star ratings.\n",
    "\n",
    "This function is passed to the `score()` function, which will receive input from the grader, run your function with that input, report the results back to the grader, and print out the score the grader returned.  Depending on how you constructed your estimator, you may be able to pass the predict method directly to the `score()` function.  If not, you will need to write a small wrapper function to mediate the data types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bag_of_words_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a linear model predicting the star rating based on the count of the words in each document (bag-of-words model).  Use a [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) or [`HashingVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer) to produce a feature matrix giving the counts of each word in each review.  Feed this in to linear model, such as `Ridge` or `SGDRegressor`, to predict the number of stars from each review.\n",
    "\n",
    "**Hints**:\n",
    "1. Don't forget to use tokenization!  This is important for good performance but it is also the most expensive step.  Try vectorizing as a first initial step and then running grid-search and cross-validation only on of this pre-processed data.  `CountVectorizer` has to memorize the mapping between words and the index to which it is assigned.  This is linear in the size of the vocabulary.  The `HashingVectorizer` does not have to remember this mapping and will lead to much smaller models.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [row['text'] for row in data]\n",
    "X = CountVectorizer().fit_transform(text)\n",
    "\n",
    "# Now, this can be run with many different parameters\n",
    "# without needing to retrain the vectorizer:\n",
    "model.fit(X, stars, hyperparameter=something)\n",
    "```\n",
    "\n",
    "2. Try choosing different values for `min_df` (minimum document frequency cutoff) and `max_df` in `CountVectorizer`.  Setting `min_df` to zero admits rare words which might only appear once in the entire corpus.  This is both prone to overfitting and makes your data unmanageably large.  Don't forget to use cross-validation or to select the right value.  Notice that `HashingVectorizer` doesn't support `min_df`  and `max_df`.  However, it's not hard to roll your own transformer that solves for these.\n",
    "\n",
    "3. Try using [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) or [`RidgeCV`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html#sklearn.linear_model.RidgeCV).  If the memory footprint is too big, try switching to [Stochastic Gradient Descent](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor) You might find that even ordinary linear regression fails due to the data size.  Don't forget to use [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) to determine the regularization parameter!  How do the regularization parameter `alpha` and the values of `min_df` and `max_df` from `CountVectorizer` change the answer?\n",
    "\n",
    "4. You will likely pick up several hyperparameters between the tokenization step and the regularization of the estimator.  While is is more strictly correct to do a grid search over all of them at once, this can take a long time. Quite often, doing a grid search over a single hyperparameter at a time can produce similar results.  Alternatively, the grid search may be done over a smaller subset of the data, as long as it is representative of the whole.\n",
    "\n",
    "5. Finally, assemble a pipeline that will transform the data from records all the way to predictions.  This will allow you to submit its predict method to the grader for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:37:50.723166Z",
     "start_time": "2018-10-11T17:37:50.716047Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn import base\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as ENG_stopwords \n",
    "import string\n",
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer,HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "punctuations = string.punctuation\n",
    "import pickle\n",
    "import scipy.sparse\n",
    "import collections\n",
    "import ujson as json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T05:32:31.156502Z",
     "start_time": "2018-10-09T05:32:31.150370Z"
    }
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T13:32:42.144770Z",
     "start_time": "2018-10-09T13:32:39.505097Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T01:21:12.990818Z",
     "start_time": "2018-10-10T01:21:12.027Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_json('yelp_train_academic_dataset_review_reduced.json.gz',lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T13:35:58.297216Z",
     "start_time": "2018-10-09T13:35:58.169396Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_train, df_test = train_test_split(df[['text','stars']],\\\n",
    "#                                      stratify=df['stars'],\\\n",
    "#                                      test_size=0.2,\\\n",
    "#                                      random_state=18)\n",
    "\n",
    "# df_train_y=df_train.stars\n",
    "# df_train_X=df_train.drop('stars',axis=1)\n",
    "# df_test_X=df_test.drop('stars',axis=1)\n",
    "# df_test_y=df_test.stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T18:18:36.253877Z",
     "start_time": "2018-10-09T18:18:36.247356Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_train_y.to_pickle('df_train_y')\n",
    "\n",
    "# df_train_X.to_pickle('df_train_X')\n",
    "\n",
    "# df_test_X.to_pickle('df_test_X')\n",
    "\n",
    "# df_test_y.to_pickle('df_test_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:35:32.751836Z",
     "start_time": "2018-10-11T14:35:29.934101Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_y=pd.read_pickle('df_train_y')\n",
    "df_train_X=pd.read_pickle('df_train_X')\n",
    "df_test_X=pd.read_pickle('df_test_X')\n",
    "df_test_y=pd.read_pickle('df_test_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:35:37.089750Z",
     "start_time": "2018-10-11T14:35:37.082680Z"
    }
   },
   "outputs": [],
   "source": [
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_names):\n",
    "        self.col_names = col_names  # We will need these in transform()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if not isinstance(X,pd.DataFrame):\n",
    "            X=pd.DataFrame(X)\n",
    "        alist=X[self.col_names].values.tolist()\n",
    "        if isinstance(alist[0], (list,)):\n",
    "            return sum(alist,[])\n",
    "        return alist\n",
    "        # Return an array with the same number of rows as X and one\n",
    "        # column for each in self.col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:35:37.554865Z",
     "start_time": "2018-10-11T14:35:37.548379Z"
    }
   },
   "outputs": [],
   "source": [
    "class PreprocessorTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_names='text'):\n",
    "        self.col_names = col_names  # We will need these in transform()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        results=[]\n",
    "        for i in X:\n",
    "            \n",
    "            cleaned=preprocessor(i).split()\n",
    "            removed=remove_stop_pun(cleaned)\n",
    "            results.append(removed)\n",
    "            \n",
    "        \n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:54:35.603342Z",
     "start_time": "2018-10-11T14:54:35.597169Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):     \n",
    "    return text.strip().lower()\n",
    "nltk_stopwords_mod=set(nltk.corpus.stopwords.words('english'))\n",
    "def remove_stop_pun(text):\n",
    "#     results=[]\n",
    "    return [tok for tok in text if (tok not in punctuations and tok not in nltk_stopwords_mod)]\n",
    "#     for tok in text:\n",
    "#         if not bool(re.search(r'\\d', tok)):\n",
    "#             if tok not in punctuations:\n",
    "#                 if tok not in nltk.corpus.stopwords.words('english'):\n",
    "#                     results.append(tok)\n",
    "            \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T15:03:14.343162Z",
     "start_time": "2018-10-11T15:03:14.339565Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('yelp_stopwords.txt','rb') as file:\n",
    "    yelp_stopwords=file.readlines()[0].decode('utf-8').split('\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T15:04:09.343452Z",
     "start_time": "2018-10-11T15:04:09.338975Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk_stopwords_mod.update(yelp_stopwords)\n",
    "nltk_stopwords_mod.update(['bla','well'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:51:11.338184Z",
     "start_time": "2018-10-11T14:51:11.333448Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_tokenizer(text):\n",
    "    if not isinstance(text, str):\n",
    "        text=str(text)\n",
    "    cleaned=preprocessor(text).split()\n",
    "    removed=remove_stop_pun(cleaned)\n",
    "    return removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:59:27.080092Z",
     "start_time": "2018-10-11T14:59:26.445179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-10-11 14:59:26--  https://raw.githubusercontent.com/Jigar24/Yelp-Topic-Modelling/master/stopwords.txt\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.20.133\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.20.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 3622 (3.5K) [text/plain]\r\n",
      "Saving to: 'yelp_stopwords.txt'\r\n",
      "\r\n",
      "\r",
      " 0% [                                       ] 0           --.-K/s              \r",
      "100%[======================================>] 3,622       --.-K/s   in 0s      \r\n",
      "\r\n",
      "2018-10-11 14:59:26 (75.5 MB/s) - 'yelp_stopwords.txt' saved [3622/3622]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#!wget -O yelp_stopwords.txt https://raw.githubusercontent.com/Jigar24/Yelp-Topic-Modelling/master/stopwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T14:56:37.318509Z",
     "start_time": "2018-10-11T14:56:37.314051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer('ezzyujdouig4p gyb3pv_a this is a test bla bla well')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T04:19:16.194674Z",
     "start_time": "2018-10-10T04:19:16.192133Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove_stop_pun(preprocessor(fst_stp[0]).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T02:54:05.483097Z",
     "start_time": "2018-10-11T02:54:05.477109Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessor(astring):\n",
    "    #text=alist[0]\n",
    "    #astring = str(astring)\n",
    "    astring = re.sub('<[^>]*>', '', astring)\n",
    "    astring=re.sub(\"\\S*\\d\\S*\", \"\", astring).strip()\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', astring)\n",
    "    astring = re.sub('[\\W]+', ' ', astring.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    return astring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T16:37:19.644388Z",
     "start_time": "2018-10-09T16:37:19.640579Z"
    }
   },
   "outputs": [],
   "source": [
    "col_select=ColumnSelectTransformer('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T16:37:20.022007Z",
     "start_time": "2018-10-09T16:37:20.017828Z"
    }
   },
   "outputs": [],
   "source": [
    "fst_stp=col_select.fit_transform(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T16:38:16.788403Z",
     "start_time": "2018-10-09T16:38:16.785159Z"
    }
   },
   "outputs": [],
   "source": [
    "process=PreprocessorTransformer('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T16:38:17.193226Z",
     "start_time": "2018-10-09T16:38:17.189419Z"
    }
   },
   "outputs": [],
   "source": [
    "snd_stp=process.fit_transform(fst_stp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T16:58:52.168472Z",
     "start_time": "2018-10-09T16:58:52.164168Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=pre_tokenizer)\n",
    "#cv1 = CountVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T16:59:02.536486Z",
     "start_time": "2018-10-09T16:59:02.528426Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens=cv.fit_transform(fst_stp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T17:07:28.943753Z",
     "start_time": "2018-10-09T17:07:28.939541Z"
    }
   },
   "outputs": [],
   "source": [
    "rigid=linear_model.Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T17:07:54.226923Z",
     "start_time": "2018-10-09T17:07:54.219212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rigid.fit(tokens,df_train_y[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T16:59:05.516958Z",
     "start_time": "2018-10-09T16:59:05.511939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 ... 1 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 1 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(tokens.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T16:30:27.514991Z",
     "start_time": "2018-10-09T16:30:27.508881Z"
    }
   },
   "outputs": [],
   "source": [
    "t=PreprocessorTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T16:30:30.104633Z",
     "start_time": "2018-10-09T16:30:30.071510Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_temp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-9de077e88b32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_temp' is not defined"
     ]
    }
   ],
   "source": [
    "t.fit_transform(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T15:04:48.490253Z",
     "start_time": "2018-10-09T15:04:37.777743Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_lg',disable=['ner', 'parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T16:20:59.956867Z",
     "start_time": "2018-10-09T16:20:59.950859Z"
    }
   },
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    tokens = nlp.pipe(sentence)\n",
    "#     _tokens = []\n",
    "#     for tok in tokens:\n",
    "#         if tok.is_stop or tok.is_punct:\n",
    "#             continue\n",
    "#         else:\n",
    "#             if tok.lemma_ != \"-PRON-\":\n",
    "#                 _tokens.append(tok.lemma_.lower().strip())\n",
    "#             else:\n",
    "#                 _tokens.append(tok.lower_)\n",
    "        \n",
    "    tokens = [tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_ for tok in tokens]\n",
    "    tokens = [tok for tok in tokens if (tok not in stopwords and tok not in punctuations)]     \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T17:12:56.193617Z",
     "start_time": "2018-10-09T17:12:56.188331Z"
    }
   },
   "outputs": [],
   "source": [
    "class SpacyTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_names='text'):\n",
    "        self.col_names = col_names  # We will need these in transform()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if isinstance(X,pd.DataFrame):\n",
    "            X=X[self.col_names].values.tolist()\n",
    "        #print(X)\n",
    "        return [spacy_tokenizer(i) for i in X]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1)) \n",
    "classifier = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T18:06:31.775078Z",
     "start_time": "2018-10-09T18:06:31.579236Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T18:13:13.994101Z",
     "start_time": "2018-10-09T18:13:13.906345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vagrant/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T20:14:17.703505Z",
     "start_time": "2018-10-11T20:14:17.652964Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "#from nltk.corpus import stopwords, words\n",
    "bag_of_words_est3 = Pipeline([\n",
    "    ('text',ColumnSelectTransformer(['text'])),\n",
    "    #('strip_space',PreprocessorTransformer()),\n",
    "    #('vectorizer',CountVectorizer()),\n",
    "    ('hvect', HashingVectorizer(norm='l2',tokenizer = pre_tokenizer,stop_words=nltk_stopwords_mod)),\n",
    "    #('hvect', HashingVectorizer(norm='l2',stop_words=nltk.corpus.stopwords.words('english'))),\n",
    "    #('vectorizer',CountVectorizer(tokenizer = pre_tokenizer)),\n",
    "    ('Ridge', linear_model.Ridge(alpha = 0.2))\n",
    "])\n",
    "#bag_of_words_est.fit(data, stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T18:48:41.658802Z",
     "start_time": "2018-10-09T18:48:41.642123Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# pkl_filename = \"hash_model.pkl\"  \n",
    "# with open(pkl_filename, 'wb') as file:  \n",
    "#     pickle.dump(bag_of_words_est2, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T18:14:44.678074Z",
     "start_time": "2018-10-09T18:14:44.558955Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(pkl_filename, 'rb') as file:  \n",
    "    bag_of_words_est = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T20:14:23.064389Z",
     "start_time": "2018-10-11T20:14:20.760896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('text', ColumnSelectTransformer(col_names=['text'])), ('hvect', HashingVectorizer(alternate_sign=True, analyzer='word', binary=False,\n",
       "         decode_error='strict', dtype=<class 'numpy.float64'>,\n",
       "         encoding='utf-8', input='content', lowercase=True,\n",
       "         n_features=1048576, ngram_...it_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_est3.fit(df_train_X[:1000],df_train_y[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T18:24:29.416127Z",
     "start_time": "2018-10-09T18:24:29.394454Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred=bag_of_words_est2.predict(df_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T19:04:30.018725Z",
     "start_time": "2018-10-09T19:04:30.014014Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vectorizer__max_df': (0.5, 0.75, 1.0),\n",
    "    'vectorizer__min_df': (0.01,0.05),\n",
    "    'vectorizer__max_features': (None, 5000, 10000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    #'clf__max_iter': (5,),\n",
    "    'Ridge__alpha': (0.00001, 0.3)\n",
    "    #'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag_of_words_est3 = Pipeline([\n",
    "#     ('text',ColumnSelectTransformer(['text'])),\n",
    "#     #('strip_space',PreprocessorTransformer()),\n",
    "#     #('vectorizer',CountVectorizer()),\n",
    "#     #('hvect', HashingVectorizer(norm='l2',stop_words=nltk.corpus.stopwords.words('english'))),\n",
    "#     ('vectorizer',CountVectorizer(tokenizer = pre_tokenizer)),\n",
    "#     ('Ridge', linear_model.Ridge(alpha = 0.1))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T20:22:20.815445Z",
     "start_time": "2018-10-11T20:14:42.826110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('text', ColumnSelectTransformer(col_names=['text'])), ('hvect', HashingVectorizer(alternate_sign=True, analyzer='word', binary=False,\n",
       "         decode_error='strict', dtype=<class 'numpy.float64'>,\n",
       "         encoding='utf-8', input='content', lowercase=True,\n",
       "         n_features=1048576, ngram_...it_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_est3.fit(df_train_X,df_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_filename = \"hash_model2.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(bag_of_words_est3, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T19:04:07.216809Z",
     "start_time": "2018-10-09T19:04:07.203913Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'param_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-201-e61da1a0a244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag_of_words_est3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'param_grid' is not defined"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(bag_of_words_est3, param_grid=param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T18:24:49.890222Z",
     "start_time": "2018-10-09T18:24:49.882338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.27827595, 3.6560669 , 3.65202657, 4.41642968, 3.45992641,\n",
       "       4.19996179, 3.89467914, 3.63441835, 3.7703331 , 4.64428548,\n",
       "       3.90513529, 3.48517155, 4.4345296 , 4.36572501, 3.16345258,\n",
       "       3.99171452, 3.60752442, 3.87664112, 3.62246333, 4.23079934,\n",
       "       3.8147538 , 3.81146937, 3.75912095, 3.71261672, 4.23989826,\n",
       "       2.94474994, 3.35020055, 3.17176146, 3.92071311, 3.63209517,\n",
       "       3.5885292 , 3.70242953, 3.69104267, 4.09912584, 4.07969628,\n",
       "       3.96249576, 3.11019962, 3.52979469, 3.51629881, 4.21057077,\n",
       "       3.600051  , 2.89419729, 3.51314514, 3.90138743, 4.53044089,\n",
       "       4.01160075, 3.33016271, 4.27964687, 3.4369498 , 4.26310209,\n",
       "       4.27375108, 4.03590278, 3.65633296, 3.55806772, 3.98209199,\n",
       "       4.35543886, 3.96320675, 3.89328294, 4.05817187, 3.00605404,\n",
       "       3.17682462, 3.95521873, 3.93447817, 3.19445001, 3.76652267,\n",
       "       3.04702488, 3.1389399 , 4.13334395, 2.72894043, 3.366292  ,\n",
       "       3.86964989, 4.15000206, 2.93155745, 3.07105869, 4.55786221,\n",
       "       3.58543107, 3.64073404, 4.05427991, 3.42151306, 4.1419399 ,\n",
       "       3.94070339, 4.49479849, 3.80069128, 3.73464146, 3.76560366,\n",
       "       3.9948373 , 4.4382191 , 3.87498789, 3.94879081, 3.82928112,\n",
       "       3.16545037, 3.99386048, 2.95555414, 3.30237283, 4.13089075,\n",
       "       3.97985761, 3.84189277, 3.75908424, 3.14147088, 3.6941628 ])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T19:02:47.758224Z",
     "start_time": "2018-10-09T19:02:47.614578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.363"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(df_test_y[:1000].values,np.round(bag_of_words_est3.predict(df_test_X[:1000]),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl_filename = \"hash_model.pkl\"  \n",
    "# with open(pkl_filename, 'wb') as file:  \n",
    "#     pickle.dump(bag_of_words_est, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipe = Pipeline([('cst',ColumnSelectTransformer(['categories'])),\n",
    "                     ('DictEn',DictEncoder()),\n",
    "                     ('DictVec',DictVectorizer(sparse=False)),\n",
    "                     ('Ridge', linear_model.Ridge(alpha = 0.1))\n",
    "                     \n",
    "        # ColumnSelectTransformer\n",
    "        # KNeighborsRegressor\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T19:44:33.325000Z",
     "start_time": "2018-10-09T19:44:32.087990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.3194332860293514\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('nlp__bag_of_words_model', bag_of_words_est3.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T20:10:14.101478Z",
     "start_time": "2018-10-11T20:10:12.104944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.2619672911699913\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('nlp__bag_of_words_model', bag_of_words_est3.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is key for good linear regression. Previously, we used the count as the normalization scheme.  Add in a normalization transformer to your pipeline to improve the score.  Try some of these:\n",
    "\n",
    "1. You can use the \"does this word present in this document\" as a normalization scheme, which means the values are always 1 or 0.  So we give no additional weight to the presence of the word multiple times.\n",
    "\n",
    "2. Try using the log of the number of counts (or more precisely, $log(x+1)$). This is often used because we want the repeated presence of a word to count for more but not have that effect tapper off.\n",
    "\n",
    "3. [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is a common normalization scheme used in text processing.  Use the [`TfidfTransformer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer). There are options for using `idf` and taking the logarithm of `tf`.  Do these significantly affect the result?\n",
    "\n",
    "Finally, if you can't decide which one is better, don't forget that you can combine models with a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T19:44:41.545666Z",
     "start_time": "2018-10-09T19:44:40.354816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.2183051415982085\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('nlp__normalized_model', bag_of_words_est3.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T20:11:18.345663Z",
     "start_time": "2018-10-11T20:11:16.737246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.1652436357642129\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('nlp__normalized_model', bag_of_words_est3.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.score('nlp__normalized_model', normalized_est.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bigram_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a bigram model, we'll consider both single words and pairs of consecutive words that appear.  This is going to be a much higher dimensional problem (large $p$) so you should be careful about overfitting.\n",
    "\n",
    "Sometimes, reducing the dimension can be useful.  Because we are dealing with a sparse matrix, we have to use [`TruncatedSVD`](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD).  If we reduce the dimensions, we can use a more sophisticated models than linear ones.\n",
    "\n",
    "As before, memory problems can crop up due to the engineering constraints. Playing with the number of features, using the `HashingVectorizer`, incorporating `min_df` and `max_df` limits, and handling stop-words in some way are all methods of addressing this issue. If you are using `CountVectorizer`, it is possible to run it with a fixed vocabulary (based on a training run, for instance). Check the documentation.\n",
    "\n",
    "**A side note on multi-stage model evaluation:** When your model consists of a pipeline with several stages, it can be worthwhile to evaluate which parts of the pipeline have the greatest impact on the overall accuracy (or other metric) of the model. This allows you to focus your efforts on improving the important algorithms, and leaving the rest \"good enough\".\n",
    "\n",
    "One way to accomplish this is through ceiling analysis, which can be useful when you have a training set with ground truth values at each stage. Let's say you're training a model to extract image captions from websites and return a list of names that were in the caption. Your overall accuracy at some point reaches 70%. You can try manually giving the model what you know are the correct image captions from the training set, and see how the accuracy improves (maybe up to 75%). Alternatively, giving the model the perfect name parsing for each caption increases accuracy to 90%. This indicates that the name parsing is a much more promising target for further work, and the caption extraction is a relatively smaller factor in the overall performance.\n",
    "\n",
    "If you don't know the right answers at different stages of the pipeline, you can still evaluate how important different parts of the model are to its performance by changing or removing certain steps while keeping everything else constant. You might try this kind of analysis to determine how important adding stopwords and stemming to your NLP model actually is, and how that importance changes with parameters like the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T18:51:22.144027Z",
     "start_time": "2018-10-09T18:51:20.723114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.2149417045378152\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('nlp__bigram_model', bag_of_words_est2.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T19:45:04.950947Z",
     "start_time": "2018-10-09T19:45:04.944815Z"
    }
   },
   "outputs": [],
   "source": [
    "bag_of_words_est4 = Pipeline([\n",
    "    ('text',ColumnSelectTransformer(['text'])),\n",
    "    #('strip_space',PreprocessorTransformer()),\n",
    "    #('vectorizer',CountVectorizer()),\n",
    "    ('hvect', HashingVectorizer(norm='l2',ngram_range=(1,2),tokenizer = pre_tokenizer)),\n",
    "    #('hvect', HashingVectorizer(norm='l2',stop_words=nltk.corpus.stopwords.words('english'))),\n",
    "    #('vectorizer',CountVectorizer(tokenizer = pre_tokenizer)),\n",
    "    ('Ridge', linear_model.Ridge(alpha = 0.2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T19:51:14.838622Z",
     "start_time": "2018-10-09T19:45:08.301956Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('text', ColumnSelectTransformer(col_names=['text'])), ('hvect', HashingVectorizer(alternate_sign=True, analyzer='word', binary=False,\n",
       "         decode_error='strict', dtype=<class 'numpy.float64'>,\n",
       "         encoding='utf-8', input='content', lowercase=True,\n",
       "         n_features=1048576, ngram_...it_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_est4.fit(df_train_X,df_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T19:52:19.191905Z",
     "start_time": "2018-10-09T19:52:19.174550Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pkl_filename = \"bigram_model.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(bag_of_words_est4, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T19:51:55.981523Z",
     "start_time": "2018-10-09T19:51:54.547567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.2534451191274427\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('nlp__bigram_model', bag_of_words_est4.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.score('nlp__bigram_model', bigram_est.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## food_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look over all reviews of restaurants.  You can determine which businesses are restaurants by looking in the `yelp_train_academic_dataset_business.json.gz` file from the ml project or downloaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T19:53:21.570922Z",
     "start_time": "2018-10-09T19:53:20.289904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dataincubator-course/mldata/yelp_train_academic_dataset_business.json.gz to ./yelp_train_academic_dataset_business.json.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' --include 'yelp_train_academic_dataset_business.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T01:11:29.244609Z",
     "start_time": "2018-10-10T01:11:25.205781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://dataincubator-course/mldata/yelp_train_academic_dataset_review.json.gz to ./yelp_train_academic_dataset_review.json.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync s3://dataincubator-course/mldata/ . --exclude '*' --include 'yelp_train_academic_dataset_review.json.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T01:11:43.871444Z",
     "start_time": "2018-10-10T01:11:43.856150Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gzip' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-7308366c8ae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yelp_train_academic_dataset_review.json.gz'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mbusiness_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gzip' is not defined"
     ]
    }
   ],
   "source": [
    "with gzip.open('yelp_train_academic_dataset_review.json.gz') as f:\n",
    "    business_data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of this file corresponds to a single business.  The category key gives a list of categories for each; take all where \"Restaurants\" appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T20:29:44.874725Z",
     "start_time": "2018-10-10T20:29:43.286767Z"
    }
   },
   "outputs": [],
   "source": [
    "business_data=pd.read_json('yelp_train_academic_dataset_business.json.gz',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T20:29:43.284368Z",
     "start_time": "2018-10-10T20:29:08.405698Z"
    }
   },
   "outputs": [],
   "source": [
    "review_data=pd.read_json('yelp_train_academic_dataset_review.json.gz',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T20:29:44.883070Z",
     "start_time": "2018-10-10T20:29:44.876987Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1012913"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(review_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T04:20:09.572314Z",
     "start_time": "2018-10-10T04:20:09.282569Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9e112543b788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T01:12:31.727618Z",
     "start_time": "2018-10-10T01:12:31.706599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2007-05-17</td>\n",
       "      <td>15SdjuK7DmYqUAj6rjGowg</td>\n",
       "      <td>5</td>\n",
       "      <td>dr. goldberg offers everything i look for in a...</td>\n",
       "      <td>review</td>\n",
       "      <td>Xqd0DzHaiyRqVH3WRG7hzg</td>\n",
       "      <td>{'funny': 0, 'useful': 2, 'cool': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2010-03-22</td>\n",
       "      <td>RF6UnRTtG7tWMcrO2GEoAg</td>\n",
       "      <td>2</td>\n",
       "      <td>Unfortunately, the frustration of being Dr. Go...</td>\n",
       "      <td>review</td>\n",
       "      <td>H1kH6QZV7Le4zqTRNxoZow</td>\n",
       "      <td>{'funny': 0, 'useful': 2, 'cool': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2012-02-14</td>\n",
       "      <td>-TsVN230RCkLYKBeLsuz7A</td>\n",
       "      <td>4</td>\n",
       "      <td>Dr. Goldberg has been my doctor for years and ...</td>\n",
       "      <td>review</td>\n",
       "      <td>zvJCcrpm2yOZrxKffwGQLA</td>\n",
       "      <td>{'funny': 0, 'useful': 1, 'cool': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2012-03-02</td>\n",
       "      <td>dNocEAyUucjT371NNND41Q</td>\n",
       "      <td>4</td>\n",
       "      <td>Been going to Dr. Goldberg for over 10 years. ...</td>\n",
       "      <td>review</td>\n",
       "      <td>KBLW4wJA_fwoWmMhiHRVOA</td>\n",
       "      <td>{'funny': 0, 'useful': 0, 'cool': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vcNAWiLM4dR7D2nwwJ7nCA</td>\n",
       "      <td>2012-05-15</td>\n",
       "      <td>ebcN2aqmNUuYNoyvQErgnA</td>\n",
       "      <td>4</td>\n",
       "      <td>Got a letter in the mail last week that said D...</td>\n",
       "      <td>review</td>\n",
       "      <td>zvJCcrpm2yOZrxKffwGQLA</td>\n",
       "      <td>{'funny': 0, 'useful': 2, 'cool': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id       date               review_id  stars  \\\n",
       "0  vcNAWiLM4dR7D2nwwJ7nCA 2007-05-17  15SdjuK7DmYqUAj6rjGowg      5   \n",
       "1  vcNAWiLM4dR7D2nwwJ7nCA 2010-03-22  RF6UnRTtG7tWMcrO2GEoAg      2   \n",
       "2  vcNAWiLM4dR7D2nwwJ7nCA 2012-02-14  -TsVN230RCkLYKBeLsuz7A      4   \n",
       "3  vcNAWiLM4dR7D2nwwJ7nCA 2012-03-02  dNocEAyUucjT371NNND41Q      4   \n",
       "4  vcNAWiLM4dR7D2nwwJ7nCA 2012-05-15  ebcN2aqmNUuYNoyvQErgnA      4   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  dr. goldberg offers everything i look for in a...  review   \n",
       "1  Unfortunately, the frustration of being Dr. Go...  review   \n",
       "2  Dr. Goldberg has been my doctor for years and ...  review   \n",
       "3  Been going to Dr. Goldberg for over 10 years. ...  review   \n",
       "4  Got a letter in the mail last week that said D...  review   \n",
       "\n",
       "                  user_id                                 votes  \n",
       "0  Xqd0DzHaiyRqVH3WRG7hzg  {'funny': 0, 'useful': 2, 'cool': 1}  \n",
       "1  H1kH6QZV7Le4zqTRNxoZow  {'funny': 0, 'useful': 2, 'cool': 0}  \n",
       "2  zvJCcrpm2yOZrxKffwGQLA  {'funny': 0, 'useful': 1, 'cool': 1}  \n",
       "3  KBLW4wJA_fwoWmMhiHRVOA  {'funny': 0, 'useful': 0, 'cool': 0}  \n",
       "4  zvJCcrpm2yOZrxKffwGQLA  {'funny': 0, 'useful': 2, 'cool': 1}  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T01:23:44.517292Z",
     "start_time": "2018-10-10T01:23:44.513063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37938"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(business_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T20:29:44.982069Z",
     "start_time": "2018-10-10T20:29:44.886395Z"
    }
   },
   "outputs": [],
   "source": [
    "business_data_mod=business_data[business_data['categories'].apply(str).str.contains(\"Restaurants\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T20:29:44.988734Z",
     "start_time": "2018-10-10T20:29:44.985018Z"
    }
   },
   "outputs": [],
   "source": [
    "restaurant_ids = business_data_mod['business_id'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T20:29:44.997417Z",
     "start_time": "2018-10-10T20:29:44.992034Z"
    }
   },
   "outputs": [],
   "source": [
    "assert len(business_data_mod) == 12876"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"business_id\" here is the same as in the review data.  Use this to extract the review text for all reviews of restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T15:06:11.027894Z",
     "start_time": "2018-10-11T15:06:09.751192Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "restaurant_reviews=pd.DataFrame()\n",
    "restaurant_reviews = review_data[review_data.business_id.isin(restaurant_ids)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T15:06:11.035396Z",
     "start_time": "2018-10-11T15:06:11.030686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574278"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(restaurant_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T04:20:45.643255Z",
     "start_time": "2018-10-10T04:20:45.632597Z"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-10b5ce0c6083>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestaurant_reviews\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m143361\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert len(restaurant_reviews) == 143361"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find collocations --- that is, bigrams that are \"special\" and appear more often than you'd expect from chance. We can think of the corpus as defining an empirical distribution over all *n*-grams.  We can find word pairs that are unlikely to occur consecutively based on the underlying probability of their words. Mathematically, if $p(w)$ be the probability of a word $w$ and $p(w_1 w_2)$ is the probability of the bigram $w_1 w_2$, then we want to look at word pairs $w_1 w_2$ where the statistic\n",
    "\n",
    "  $$ \\frac{p(w_1 w_2)}{p(w_1) p(w_2)} $$\n",
    "\n",
    "is high.  Return the top 100 (mostly food) bigrams with this statistic with the 'right' prior factor (see below).\n",
    "\n",
    "Estimating the probabilities is simply a matter of counting, and there are number of approaches that will work.  One is to use one of the tokenizers to count up how many times each word and each bigram appears in each review, and then sum those up over all reviews.  You might want to know that the `CountVectorizer` has a `.get_feature_names()` method which gives the string associated with each column.  (Question for thought: Why doesn't the `HashingVectorizer` have a similar method?)\n",
    "\n",
    "*Questions:* This statistic is a ratio and problematic when the denominator is small.  We can fix this by applying Bayesian smoothing to $p(w)$ (i.e. mixing the empirical distribution with the uniform distribution over the vocabulary).\n",
    "\n",
    "1. How does changing this smoothing parameter affect the word pairs you get qualitatively?\n",
    "\n",
    "2. We can interpret the smoothing parameter as adding a constant number of occurrences of each word to our distribution.  Does this help you determine set a reasonable value for this 'prior factor'?\n",
    "\n",
    "3. For fun: also check out [Amazon's Statistically Improbable Phrases](http://en.wikipedia.org/wiki/Statistically_Improbable_Phrases).\n",
    "\n",
    "*Implementation note:*\n",
    "As you adjust the size of the Bayesian smoothing parameter, you will notice first nonsense phrases being removed and then legitimate bigrams being removed, leaving you with only generic bigrams.  The goal is to find a value of the smoothing parameter between these two transitions.\n",
    "\n",
    "The reference solution is not an aggressive filterer: it errors in favor of leaving apparently nonsensical words. On further consideration, many of these are actually somewhat meaningful. The smoothing parameter chosen in the reference solution is equivalent to giving each word 30 previous appearances prior to considering this data.  This was chosen by generating a list of bigrams for a range of smoothing parameters and seeing how many of the bigrams were shared between neighboring values.  When the shared fraction reached 95%, we judged the solution to have converged.  Note that `min_df` should not be set too high, where it could exclude these borderline words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T20:29:45.767961Z",
     "start_time": "2018-10-10T20:29:45.642996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JwUE5GmEO-sH1FuwJgKBlQ</td>\n",
       "      <td>2009-05-03</td>\n",
       "      <td>9uHZyOu5CTCDl1L6cfvOCA</td>\n",
       "      <td>4</td>\n",
       "      <td>Good truck stop dining at the right price. We ...</td>\n",
       "      <td>review</td>\n",
       "      <td>p4ySEi8PEli0auZGBsy6gA</td>\n",
       "      <td>{'funny': 0, 'useful': 0, 'cool': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JwUE5GmEO-sH1FuwJgKBlQ</td>\n",
       "      <td>2009-05-04</td>\n",
       "      <td>ow1c4Lcl3ObWxDC2yurwjQ</td>\n",
       "      <td>4</td>\n",
       "      <td>If you like lot lizards, you'll love the Pine ...</td>\n",
       "      <td>review</td>\n",
       "      <td>ZYaumz29bl9qHpu-KVtMGA</td>\n",
       "      <td>{'funny': 6, 'useful': 0, 'cool': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JwUE5GmEO-sH1FuwJgKBlQ</td>\n",
       "      <td>2010-10-30</td>\n",
       "      <td>FRTCszJWkJonDAZx3yr8FA</td>\n",
       "      <td>4</td>\n",
       "      <td>Enjoyable experience for the whole family. The...</td>\n",
       "      <td>review</td>\n",
       "      <td>SvS7NXWG2B2kFoaHaWdGfg</td>\n",
       "      <td>{'funny': 0, 'useful': 0, 'cool': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JwUE5GmEO-sH1FuwJgKBlQ</td>\n",
       "      <td>2011-02-06</td>\n",
       "      <td>qQIvtbqUujvvnJDzPSfmFA</td>\n",
       "      <td>4</td>\n",
       "      <td>One of my favorite truck stop diners with soli...</td>\n",
       "      <td>review</td>\n",
       "      <td>qOYI9O0ecMJ9VaqcM9phNw</td>\n",
       "      <td>{'funny': 0, 'useful': 0, 'cool': 0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>JwUE5GmEO-sH1FuwJgKBlQ</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>4iPPOQIo5Mr1NAUPUgCUrQ</td>\n",
       "      <td>4</td>\n",
       "      <td>Only went here once about a year and a half ag...</td>\n",
       "      <td>review</td>\n",
       "      <td>EEYwj6_t1OT5WQGypqEPNg</td>\n",
       "      <td>{'funny': 0, 'useful': 0, 'cool': 0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               business_id       date               review_id  stars  \\\n",
       "8   JwUE5GmEO-sH1FuwJgKBlQ 2009-05-03  9uHZyOu5CTCDl1L6cfvOCA      4   \n",
       "9   JwUE5GmEO-sH1FuwJgKBlQ 2009-05-04  ow1c4Lcl3ObWxDC2yurwjQ      4   \n",
       "10  JwUE5GmEO-sH1FuwJgKBlQ 2010-10-30  FRTCszJWkJonDAZx3yr8FA      4   \n",
       "11  JwUE5GmEO-sH1FuwJgKBlQ 2011-02-06  qQIvtbqUujvvnJDzPSfmFA      4   \n",
       "12  JwUE5GmEO-sH1FuwJgKBlQ 2011-03-31  4iPPOQIo5Mr1NAUPUgCUrQ      4   \n",
       "\n",
       "                                                 text    type  \\\n",
       "8   Good truck stop dining at the right price. We ...  review   \n",
       "9   If you like lot lizards, you'll love the Pine ...  review   \n",
       "10  Enjoyable experience for the whole family. The...  review   \n",
       "11  One of my favorite truck stop diners with soli...  review   \n",
       "12  Only went here once about a year and a half ag...  review   \n",
       "\n",
       "                   user_id                                 votes  \n",
       "8   p4ySEi8PEli0auZGBsy6gA  {'funny': 0, 'useful': 0, 'cool': 0}  \n",
       "9   ZYaumz29bl9qHpu-KVtMGA  {'funny': 6, 'useful': 0, 'cool': 0}  \n",
       "10  SvS7NXWG2B2kFoaHaWdGfg  {'funny': 0, 'useful': 0, 'cool': 0}  \n",
       "11  qOYI9O0ecMJ9VaqcM9phNw  {'funny': 0, 'useful': 0, 'cool': 0}  \n",
       "12  EEYwj6_t1OT5WQGypqEPNg  {'funny': 0, 'useful': 0, 'cool': 0}  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T15:08:01.486497Z",
     "start_time": "2018-10-11T15:06:27.896586Z"
    }
   },
   "outputs": [],
   "source": [
    "restaurant_reviews['cleaned_text']=restaurant_reviews.text.apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T20:48:21.675621Z",
     "start_time": "2018-10-10T20:48:21.656457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8     good truck stop dining at the right price we l...\n",
       "9     if you like lot lizards you ll love the pine c...\n",
       "10    enjoyable experience for the whole family the ...\n",
       "11    one of my favorite truck stop diners with soli...\n",
       "12    only went here once about a year and a half ag...\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_reviews['cleaned_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T20:48:15.854592Z",
     "start_time": "2018-10-10T20:48:15.656518Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:20:42.690368Z",
     "start_time": "2018-10-11T16:20:41.816212Z"
    }
   },
   "outputs": [],
   "source": [
    "monogram=CountVectorizer(ngram_range=(1,1),stop_words=nltk.corpus.stopwords.words('english'),min_df=20)\n",
    "bigram=CountVectorizer(ngram_range=(2,2),stop_words=nltk.corpus.stopwords.words('english'),min_df=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T15:24:32.865788Z",
     "start_time": "2018-10-11T15:24:32.309291Z"
    }
   },
   "outputs": [],
   "source": [
    "monogram=CountVectorizer(ngram_range=(1,1),tokenizer=pre_tokenizer,min_df=20)\n",
    "bigram=CountVectorizer(ngram_range=(2,2),tokenizer=pre_tokenizer,min_df=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:31:10.110704Z",
     "start_time": "2018-10-11T16:31:10.106795Z"
    }
   },
   "outputs": [],
   "source": [
    "monogram=CountVectorizer(ngram_range=(1,1),stop_words=nltk_stopwords_mod,min_df=20)\n",
    "bigram=CountVectorizer(ngram_range=(2,2),stop_words=nltk_stopwords_mod,min_df=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:41:28.728397Z",
     "start_time": "2018-10-11T17:41:28.724114Z"
    }
   },
   "outputs": [],
   "source": [
    "monogram_tf=TfidfVectorizer(ngram_range=(1,1),stop_words=nltk_stopwords_mod,min_df=20)\n",
    "bigram_tf=TfidfVectorizer(ngram_range=(2,2),stop_words=nltk_stopwords_mod,min_df=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T15:22:35.240713Z",
     "start_time": "2018-10-11T15:22:35.232574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8     good truck stop dining at the right price we l...\n",
       "9     if you like lot lizards you ll love the pine c...\n",
       "10    enjoyable experience for the whole family the ...\n",
       "11    one of my favorite truck stop diners with soli...\n",
       "12    only went here once about a year and a half ag...\n",
       "13    great truck stop restaurant i ve had breakfast...\n",
       "14    yeah thats right a five freakin star rating fi...\n",
       "15    ate a saturday morning breakfast at the pine c...\n",
       "16    attention fans of david lynch do stop by this ...\n",
       "17    with a recent addition of a truck driver for a...\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_reviews['cleaned_text'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:21:48.999065Z",
     "start_time": "2018-10-11T16:20:49.344249Z"
    }
   },
   "outputs": [],
   "source": [
    "mono_tokens=monogram.fit_transform(restaurant_reviews['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:32:08.038547Z",
     "start_time": "2018-10-11T16:31:12.753574Z"
    }
   },
   "outputs": [],
   "source": [
    "mono_tokens=monogram.fit_transform(restaurant_reviews['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:42:49.360011Z",
     "start_time": "2018-10-11T17:41:50.757846Z"
    }
   },
   "outputs": [],
   "source": [
    "mono_tokens_tf=monogram_tf.fit_transform(restaurant_reviews['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:24:19.271996Z",
     "start_time": "2018-10-11T16:21:49.001869Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_tokens=bigram.fit_transform(restaurant_reviews['cleaned_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:24:19.271996Z",
     "start_time": "2018-10-11T16:21:49.001869Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_tokens=bigram.fit_transform(restaurant_reviews['cleaned_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:45:48.016585Z",
     "start_time": "2018-10-11T17:43:26.603041Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_tokens_tf=bigram_tf.fit_transform(restaurant_reviews['cleaned_text']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:24:27.377504Z",
     "start_time": "2018-10-11T16:24:27.330457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25822"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(monogram.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:46:42.277826Z",
     "start_time": "2018-10-11T17:46:42.244406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25468"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(monogram_tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-10-10T04:17:33.033Z"
    }
   },
   "outputs": [],
   "source": [
    "len(mono_tokens.toarray().sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T04:13:58.022973Z",
     "start_time": "2018-10-10T04:13:58.009505Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "file must have 'read' and 'readline' attributes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-3efe95d593a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtemp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mono_tokens.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: file must have 'read' and 'readline' attributes"
     ]
    }
   ],
   "source": [
    "temp=pickle.load('mono_tokens.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T04:26:44.885961Z",
     "start_time": "2018-10-10T04:26:44.881127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.CountVectorizer"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(monogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T20:57:52.425429Z",
     "start_time": "2018-10-10T20:57:33.576387Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "scipy.sparse.save_npz('mono_tokens_pre_token.npz',mono_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T20:58:05.162803Z",
     "start_time": "2018-10-10T20:57:52.428099Z"
    }
   },
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('bigram_tokens_pre_token.npz',bigram_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:24:32.907739Z",
     "start_time": "2018-10-11T16:24:32.856810Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__',\n",
       " '___',\n",
       " '____',\n",
       " '_____',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aaaaand',\n",
       " 'aaaand',\n",
       " 'aaah',\n",
       " 'aah',\n",
       " 'aaron',\n",
       " 'ab',\n",
       " 'aback',\n",
       " 'abacus',\n",
       " 'abalone',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abbey',\n",
       " 'abbreviated',\n",
       " 'abby',\n",
       " 'abc',\n",
       " 'abd',\n",
       " 'abe',\n",
       " 'aber',\n",
       " 'aberdeen',\n",
       " 'aberration',\n",
       " 'abhor',\n",
       " 'abhorrent',\n",
       " 'abide',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abit',\n",
       " 'abita',\n",
       " 'able',\n",
       " 'abnormal',\n",
       " 'abnormally',\n",
       " 'abode',\n",
       " 'abominable',\n",
       " 'abomination']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monogram.get_feature_names()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:15:24.432904Z",
     "start_time": "2018-10-11T16:15:24.373381Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':(',\n",
       " ':)',\n",
       " ';(',\n",
       " ';)',\n",
       " '=(',\n",
       " '=)',\n",
       " '__',\n",
       " '___',\n",
       " '____',\n",
       " '_____',\n",
       " '______',\n",
       " '_______',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aaaaand',\n",
       " 'aaaah',\n",
       " 'aaaand',\n",
       " 'aaah',\n",
       " 'aaahhh',\n",
       " 'aaand',\n",
       " 'aah',\n",
       " 'aahh',\n",
       " 'aahhh',\n",
       " 'aahing',\n",
       " 'aand',\n",
       " 'aaron',\n",
       " 'aarp',\n",
       " 'ab',\n",
       " 'aback',\n",
       " 'abacus',\n",
       " 'abalone',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abbey',\n",
       " 'abbreviated',\n",
       " 'abby',\n",
       " 'abc',\n",
       " 'abd',\n",
       " 'abdominal']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monogram.get_feature_names()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:22:13.596287Z",
     "start_time": "2018-10-10T03:22:13.591433Z"
    }
   },
   "outputs": [],
   "source": [
    "temp=monogram.fit_transform(restaurant_reviews['cleaned_text'].values[:4].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T03:25:33.070889Z",
     "start_time": "2018-10-10T03:25:33.065629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good truck stop dining at the right price we love coming here on the weekends when we don t feel like cooking ',\n",
       " 'if you like lot lizards you ll love the pine cone ',\n",
       " 'enjoyable experience for the whole family the wait staff was courteous and friendly the food was reasonably priced and a good value a word of advice leave room for dessert the deserters are great but huge plan to bring some home',\n",
       " 'one of my favorite truck stop diners with solid food and friendly quick service my god those desserts are huge i can t imagine eating that giant cream puff all the food we had was delicious and i love how they leave a carafe of coffee on the table love this place would definitely be back if i was in the area ']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_reviews['cleaned_text'].values[:4].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T04:30:54.673410Z",
     "start_time": "2018-10-10T04:30:47.513632Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('bigram.pkl', 'wb') as file:  \n",
    "#     pickle.dump(bigram,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T04:32:10.905002Z",
     "start_time": "2018-10-10T04:32:10.078306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00 00',\n",
       " '00 10',\n",
       " '00 11',\n",
       " '00 12',\n",
       " '00 15',\n",
       " '00 16',\n",
       " '00 20',\n",
       " '00 25',\n",
       " '00 30',\n",
       " '00 50']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T21:00:25.367226Z",
     "start_time": "2018-10-10T21:00:19.089179Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open('bigram_pre_token.pkl','wb') as file:\n",
    "#     pickle.dump(bigram,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T05:01:28.621979Z",
     "start_time": "2018-10-10T05:01:21.669966Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('bigram.pkl','rb') as file:\n",
    "    bigram=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T04:48:32.967791Z",
     "start_time": "2018-10-10T04:48:31.430903Z"
    }
   },
   "outputs": [],
   "source": [
    "mono_tokens=scipy.sparse.load_npz('mono_tokens.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T04:59:14.544326Z",
     "start_time": "2018-10-10T04:59:12.021595Z"
    }
   },
   "outputs": [],
   "source": [
    "bigram_tokens=scipy.sparse.load_npz('bigram_tokens.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T04:42:24.294737Z",
     "start_time": "2018-10-10T04:42:24.283595Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(574278, 36031)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mono_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:24:50.790337Z",
     "start_time": "2018-10-11T16:24:50.686880Z"
    }
   },
   "outputs": [],
   "source": [
    "mono_count=mono_tokens.sum(axis=0).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:24:51.278341Z",
     "start_time": "2018-10-11T16:24:51.167887Z"
    }
   },
   "outputs": [],
   "source": [
    "bi_count=bigram_tokens.sum(axis=0).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:47:09.966283Z",
     "start_time": "2018-10-11T17:47:09.760877Z"
    }
   },
   "outputs": [],
   "source": [
    "mono_count_tf=mono_tokens_tf.sum(axis=0).tolist()[0]\n",
    "\n",
    "bi_count_tf=bigram_tokens_tf.sum(axis=0).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:24:52.425997Z",
     "start_time": "2018-10-11T16:24:52.420340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[37, 43, 23, 23, 44, 26, 27, 212, 42, 23]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_count[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:24:55.540360Z",
     "start_time": "2018-10-11T16:24:55.026623Z"
    }
   },
   "outputs": [],
   "source": [
    "bi_dict=collections.Counter(dict(zip(bigram.get_feature_names(),bi_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:24:55.585906Z",
     "start_time": "2018-10-11T16:24:55.542739Z"
    }
   },
   "outputs": [],
   "source": [
    "mono_dict=collections.Counter(dict(zip(monogram.get_feature_names(),mono_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:47:20.759251Z",
     "start_time": "2018-10-11T17:47:20.336905Z"
    }
   },
   "outputs": [],
   "source": [
    "bi_dict_tf=collections.Counter(dict(zip(bigram_tf.get_feature_names(),bi_count_tf)))\n",
    "\n",
    "mono_dict_tf=collections.Counter(dict(zip(monogram_tf.get_feature_names(),mono_count_tf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T05:30:16.869804Z",
     "start_time": "2018-10-10T05:30:16.563334Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('bi_dict.pkl','wb') as file:\n",
    "    pickle.dump(bi_dict,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T05:27:29.057873Z",
     "start_time": "2018-10-10T05:27:29.033968Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open('mono_dict.pkl','rb') as file:\n",
    "#     mn_d=pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T05:01:47.865786Z",
     "start_time": "2018-10-10T05:01:47.827805Z"
    }
   },
   "outputs": [],
   "source": [
    "mono_keys=monogram.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:25:09.304673Z",
     "start_time": "2018-10-11T16:25:09.205098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('go back', 33030),\n",
       " ('happy hour', 29059),\n",
       " ('really good', 28536),\n",
       " ('pretty good', 27814),\n",
       " ('food good', 24912),\n",
       " ('first time', 24033),\n",
       " ('las vegas', 22697),\n",
       " ('next time', 22326),\n",
       " ('come back', 21760),\n",
       " ('good food', 19204)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:47:32.163139Z",
     "start_time": "2018-10-11T17:47:32.080871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food good', 3351.783143853856),\n",
       " ('great food', 2914.3366070386523),\n",
       " ('happy hour', 2810.6719081596725),\n",
       " ('good food', 2605.024382340969),\n",
       " ('food great', 2592.3777364948446),\n",
       " ('pretty good', 2543.954824402212),\n",
       " ('great service', 2377.1107304746224),\n",
       " ('love place', 2237.890852905853),\n",
       " ('las vegas', 2148.1662178591664),\n",
       " ('service great', 2123.73771882867)]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_dict_tf.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:16:38.191614Z",
     "start_time": "2018-10-11T16:16:38.167650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('food', 472694),\n",
       " ('good', 437679),\n",
       " ('place', 376964),\n",
       " ('great', 281531),\n",
       " ('service', 228799),\n",
       " ('time', 197303),\n",
       " ('back', 177252),\n",
       " ('ordered', 143858),\n",
       " ('restaurant', 142027),\n",
       " ('chicken', 131181),\n",
       " ('order', 126268),\n",
       " ('menu', 124803),\n",
       " ('nice', 119697),\n",
       " ('love', 112091),\n",
       " ('pretty', 103697),\n",
       " ('delicious', 101981),\n",
       " ('eat', 97732),\n",
       " ('pizza', 97391),\n",
       " ('vegas', 95980),\n",
       " ('sauce', 92883),\n",
       " ('cheese', 91614),\n",
       " ('bar', 88361),\n",
       " ('lunch', 86279),\n",
       " ('salad', 85845),\n",
       " ('fresh', 83571),\n",
       " ('meal', 82411),\n",
       " ('people', 82321),\n",
       " ('made', 79967),\n",
       " ('dinner', 79810),\n",
       " ('table', 79105),\n",
       " ('night', 79064),\n",
       " ('friendly', 77780),\n",
       " ('make', 77632),\n",
       " ('wait', 76882),\n",
       " ('amazing', 72467),\n",
       " ('burger', 71590),\n",
       " ('staff', 71082),\n",
       " ('bit', 66942),\n",
       " ('experience', 66400),\n",
       " ('sushi', 64262),\n",
       " ('fries', 63871),\n",
       " ('bad', 63502),\n",
       " ('side', 61566),\n",
       " ('hot', 59524),\n",
       " ('give', 59208),\n",
       " ('meat', 58177),\n",
       " ('price', 57454),\n",
       " ('happy', 57195),\n",
       " ('times', 57007),\n",
       " ('thing', 56386),\n",
       " ('drinks', 56300),\n",
       " ('small', 56281),\n",
       " ('day', 55939),\n",
       " ('tasty', 55833),\n",
       " ('taste', 55825),\n",
       " ('sandwich', 55734),\n",
       " ('server', 55401),\n",
       " ('bread', 55001),\n",
       " ('rice', 53768),\n",
       " ('worth', 53579),\n",
       " ('minutes', 53571),\n",
       " ('steak', 53450),\n",
       " ('beef', 53429),\n",
       " ('favorite', 53384),\n",
       " ('big', 52472),\n",
       " ('fried', 52144),\n",
       " ('breakfast', 52034),\n",
       " ('stars', 51472),\n",
       " ('flavor', 51437),\n",
       " ('awesome', 51303),\n",
       " ('dish', 50388),\n",
       " ('buffet', 49934),\n",
       " ('excellent', 49683),\n",
       " ('lot', 49605),\n",
       " ('long', 49533),\n",
       " ('sweet', 49408),\n",
       " ('drink', 48476),\n",
       " ('hour', 48259),\n",
       " ('soup', 47818),\n",
       " ('quality', 47702),\n",
       " ('area', 47152),\n",
       " ('location', 46456),\n",
       " ('special', 45876),\n",
       " ('find', 45864),\n",
       " ('atmosphere', 45588),\n",
       " ('prices', 45549),\n",
       " ('asked', 45358),\n",
       " ('places', 43725),\n",
       " ('dishes', 43708),\n",
       " ('wanted', 42334),\n",
       " ('recommend', 42048),\n",
       " ('pork', 41889),\n",
       " ('perfect', 41825),\n",
       " ('friends', 41733),\n",
       " ('shrimp', 41717),\n",
       " ('beer', 41629),\n",
       " ('wine', 41625),\n",
       " ('dessert', 41596),\n",
       " ('top', 41478),\n",
       " ('friend', 41217)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mono_dict.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T04:52:47.644570Z",
     "start_time": "2018-10-10T04:52:47.641925Z"
    }
   },
   "outputs": [],
   "source": [
    "t=np.array(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T04:52:58.129988Z",
     "start_time": "2018-10-10T04:52:58.126075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36031"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:48:48.939596Z",
     "start_time": "2018-10-11T17:48:48.933843Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prob(mono_dict,bi_dict,alpha):\n",
    "    results=collections.Counter()\n",
    "    \n",
    "    for b,m in zip([*bi_dict],list(map(lambda x: x.split(\" \"),[*bi_dict]))):\n",
    "        results[b]= (bi_dict[b])/((mono_dict\n",
    "                                        [m[0]]+alpha)*(mono_dict[m[1]]+alpha)*1.0)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T18:04:10.183080Z",
     "start_time": "2018-10-11T18:04:07.627026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rula bula', 0.009376564929761026),\n",
       " ('riff raff', 0.00920401941606817),\n",
       " ('reina pepiada', 0.009194342739019753),\n",
       " ('knick knacks', 0.009051696104908873),\n",
       " ('itty bitty', 0.008999222794068928)]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results=get_prob(mono_dict_tf,bi_dict_tf,25)\n",
    "final_results.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T18:04:11.559663Z",
     "start_time": "2018-10-11T18:04:11.497702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rula bula', 0.009376564929761026),\n",
       " ('riff raff', 0.00920401941606817),\n",
       " ('reina pepiada', 0.009194342739019753),\n",
       " ('knick knacks', 0.009051696104908873),\n",
       " ('itty bitty', 0.008999222794068928),\n",
       " ('pel meni', 0.00898740267601008),\n",
       " ('baskin robbins', 0.008979195391923137),\n",
       " ('ropa vieja', 0.008819885640172246),\n",
       " ('himal chuli', 0.008804720727354491),\n",
       " ('dac biet', 0.008762665220077365),\n",
       " ('krispy kreme', 0.00868209424149129),\n",
       " ('gulab jamun', 0.008536320951759414),\n",
       " ('khai hoan', 0.00845208462123658),\n",
       " ('uuu uuu', 0.008434733255585624),\n",
       " ('hoity toity', 0.008275640752286385),\n",
       " ('cien agaves', 0.008249026370630842),\n",
       " ('hodge podge', 0.008154213414545737),\n",
       " ('tammie coe', 0.00803201016088791),\n",
       " ('puerto rican', 0.00787556959447633),\n",
       " ('tutti santi', 0.007839300247338232),\n",
       " ('roka akor', 0.007819867333295032),\n",
       " ('feng shui', 0.007622866704267403),\n",
       " ('wal mart', 0.00756750504648341),\n",
       " ('leaps bounds', 0.007542607946011882),\n",
       " ('patatas bravas', 0.0075254578380124565),\n",
       " ('hu tieu', 0.007326799098555293),\n",
       " ('nooks crannies', 0.007308871582643364),\n",
       " ('molecular gastronomy', 0.007304582763787337),\n",
       " ('marche bacchus', 0.0072765739776252695),\n",
       " ('bradley ogden', 0.007167129207737031),\n",
       " ('valle luna', 0.0071207077411827),\n",
       " ('innis gunn', 0.007119002485590782),\n",
       " ('scantily clad', 0.007109165390681049),\n",
       " ('hors oeuvres', 0.007104959444308502),\n",
       " ('alain ducasse', 0.007103302220256317),\n",
       " ('vice versa', 0.007057637897243873),\n",
       " ('nanay gloria', 0.007021894277514887),\n",
       " ('yada yada', 0.006867222912791946),\n",
       " ('dueling pianos', 0.0068438817595145775),\n",
       " ('lomo saltado', 0.006833621591443256),\n",
       " ('irn bru', 0.006831578560965076),\n",
       " ('lloyd wright', 0.006812524685301394),\n",
       " ('dol sot', 0.006808627419289951),\n",
       " ('harry potter', 0.0067316429786555415),\n",
       " ('ore ida', 0.006710017596029593),\n",
       " ('artery clogging', 0.0066993058561575876),\n",
       " ('barnes noble', 0.006634161984541574),\n",
       " ('laan xang', 0.006513663192255006),\n",
       " ('bandeja paisa', 0.006499202293451496),\n",
       " ('grana padano', 0.0064580613057348525),\n",
       " ('malai kofta', 0.006427903707320612),\n",
       " ('pina colada', 0.006370603613864894),\n",
       " ('cullen skink', 0.006283185582578148),\n",
       " ('val vista', 0.006280123784473891),\n",
       " ('aguas frescas', 0.006273152777113962),\n",
       " ('pura vida', 0.006250484289342058),\n",
       " ('panna cotta', 0.006214801882498703),\n",
       " ('chicha morada', 0.006203213470667434),\n",
       " ('luc lac', 0.00619844158425162),\n",
       " ('perrier jouet', 0.0061925026097654125),\n",
       " ('turo turo', 0.006172065260462548),\n",
       " ('celine dion', 0.006152307609209933),\n",
       " ('kao tod', 0.006091841067361338),\n",
       " ('har gow', 0.00606170826743075),\n",
       " ('mille feuille', 0.006053302152836498),\n",
       " ('chino bandido', 0.0060446853684926975),\n",
       " ('nuoc mam', 0.00600978305330282),\n",
       " ('sous vide', 0.005988836317833534),\n",
       " ('rustler rooste', 0.005974419241075907),\n",
       " ('honky tonk', 0.005973933151363621),\n",
       " ('shiner bock', 0.005970945268956185),\n",
       " ('deja vu', 0.005947530598207192),\n",
       " ('lactose intolerant', 0.005933594635971731),\n",
       " ('itsy bitsy', 0.005901263387653976),\n",
       " ('porta alba', 0.005900558090094307),\n",
       " ('cabo wabo', 0.005891917642182509),\n",
       " ('doon varna', 0.005867980581676447),\n",
       " ('jean philippe', 0.005865515177521042),\n",
       " ('ritz carlton', 0.005855223480387812),\n",
       " ('hon machi', 0.005825516400945197),\n",
       " ('cardiac arrest', 0.005814408279597665),\n",
       " ('woonam jung', 0.005768071815747708),\n",
       " ('demi glace', 0.005753948037983575),\n",
       " ('bells whistles', 0.0057472918778495264),\n",
       " ('tsk tsk', 0.005742718048424777),\n",
       " ('ping pang', 0.00573854478129742),\n",
       " ('pulp fiction', 0.005731110661172247),\n",
       " ('khao soi', 0.005678417387706816),\n",
       " ('coca cola', 0.005652234433130337),\n",
       " ('bim bap', 0.005646110891947348),\n",
       " ('salo salo', 0.005631226582052177),\n",
       " ('thit nuong', 0.005626204541876117),\n",
       " ('frou frou', 0.0056171323991042015),\n",
       " ('lis doon', 0.0056003903371644095),\n",
       " ('toby keith', 0.005544945885627924),\n",
       " ('pin kaow', 0.0055449220160148135),\n",
       " ('sais quoi', 0.0055178514732730196),\n",
       " ('neeps tatties', 0.005510629461437276),\n",
       " ('casey moore', 0.0055052714758493755),\n",
       " ('womp womp', 0.0054782512697083355)]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T18:04:12.483919Z",
     "start_time": "2018-10-11T18:04:12.412625Z"
    }
   },
   "outputs": [],
   "source": [
    "final_list=[word for word, word_count in final_results.most_common(100)]# if bi_dict[word] > 30] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T18:04:14.252717Z",
     "start_time": "2018-10-11T18:04:13.688582Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  1.0\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('nlp__food_bigrams', lambda: final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:52:49.977063Z",
     "start_time": "2018-10-11T17:52:49.896153Z"
    }
   },
   "outputs": [],
   "source": [
    "final_list_tf=[word for word, word_count in final_results.most_common(1000) if bi_dict_tf[word] > 30] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T16:25:30.989697Z",
     "start_time": "2018-10-11T16:25:30.912778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hodge podge', 0.002586206896551724),\n",
       " ('himal chuli', 0.002516074923119933),\n",
       " ('hoity toity', 0.002511989038593286),\n",
       " ('roka akor', 0.0024509803921568627),\n",
       " ('knick knacks', 0.0024346353339684554),\n",
       " ('reina pepiada', 0.002420520231213873),\n",
       " ('cien agaves', 0.0024174327545114062),\n",
       " ('baskin robbins', 0.0023941343707915607),\n",
       " ('itty bitty', 0.0023631762762197544),\n",
       " ('khai hoan', 0.00234192037470726),\n",
       " ('riff raff', 0.0023106123122627496),\n",
       " ('grana padano', 0.002250768555116381),\n",
       " ('tutti santi', 0.0022428526814865287),\n",
       " ('ropa vieja', 0.00219705659036203),\n",
       " ('gulab jamun', 0.0021843145412939464),\n",
       " ('pel meni', 0.0021634615384615386),\n",
       " ('ore ida', 0.0021608643457382954),\n",
       " ('laan xang', 0.0021397177885727502),\n",
       " ('dac biet', 0.0021314848108538913),\n",
       " ('rula bula', 0.002129735389301352),\n",
       " ('hu tieu', 0.002099306316173786),\n",
       " ('innis gunn', 0.0020891747759634945),\n",
       " ('bandeja paisa', 0.0020710059171597634),\n",
       " ('tammie coe', 0.002054612937433722),\n",
       " ('chicha morada', 0.002046007520460075),\n",
       " ('alain ducasse', 0.0020257109466303076),\n",
       " ('feng shui', 0.0020161290322580645),\n",
       " ('leaps bounds', 0.0020058168689198676),\n",
       " ('dol sot', 0.002005347593582888),\n",
       " ('itsy bitsy', 0.001997245179063361),\n",
       " ('mille feuille', 0.001984126984126984),\n",
       " ('hors oeuvres', 0.001942984551680204),\n",
       " ('nooks crannies', 0.001912245323690981),\n",
       " ('marche bacchus', 0.0019116186693147964),\n",
       " ('uuu uuu', 0.0019111241197637783),\n",
       " ('celine dion', 0.0018967026553837175),\n",
       " ('nanay gloria', 0.0018359853121175031),\n",
       " ('doon varna', 0.001826417299824664),\n",
       " ('luc lac', 0.0018261504747991235),\n",
       " ('krispy kreme', 0.0018211430263547527),\n",
       " ('perrier jouet', 0.001810530436484635),\n",
       " ('deja vu', 0.0018024372085733318),\n",
       " ('molecular gastronomy', 0.001797133921486253),\n",
       " ('puerto rican', 0.0017830201618433685),\n",
       " ('vice versa', 0.0017767295597484276),\n",
       " ('patatas bravas', 0.0017691192770057124),\n",
       " ('sais quoi', 0.0017657780815675552),\n",
       " ('cullen skink', 0.0017624651770993234),\n",
       " ('woonam jung', 0.0017578554163920018),\n",
       " ('lloyd wright', 0.0017576408553852162),\n",
       " ('pura vida', 0.001747008755357832),\n",
       " ('lomo saltado', 0.0017224377095522976),\n",
       " ('nuoc mam', 0.0017130508117225384),\n",
       " ('wal mart', 0.0017058296729071602),\n",
       " ('dueling pianos', 0.001704923486360612),\n",
       " ('valle luna', 0.0016995055983713829),\n",
       " ('rustler rooste', 0.001695357329160146),\n",
       " ('bradley ogden', 0.001688215984457694),\n",
       " ('barnes noble', 0.0016874656620359469),\n",
       " ('avant garde', 0.0016702457647339538),\n",
       " ('honky tonk', 0.0016525647805393973),\n",
       " ('haricot vert', 0.0016401508938822373),\n",
       " ('kao tod', 0.0016296105069540806),\n",
       " ('irn bru', 0.0016252119841718485),\n",
       " ('lis doon', 0.0016083137448966967),\n",
       " ('khao soi', 0.0016048085901027077),\n",
       " ('malai kofta', 0.00160333493666827),\n",
       " ('aguas frescas', 0.001593874264681718),\n",
       " ('porta alba', 0.0015889992360580596),\n",
       " ('rx boiler', 0.0015822158933586489),\n",
       " ('jos andrs', 0.0015685454355327825),\n",
       " ('yadda yadda', 0.0015673469387755103),\n",
       " ('mccormick schmick', 0.0015555555555555555),\n",
       " ('yada yada', 0.001542063020214031),\n",
       " ('buen provecho', 0.0015164047422111938),\n",
       " ('artery clogging', 0.0015094433771612485),\n",
       " ('shiner bock', 0.0015051958201869916),\n",
       " ('ritz carlton', 0.0015037593984962407),\n",
       " ('womp womp', 0.0014944986987554434),\n",
       " ('rinky dink', 0.0014899576852017403),\n",
       " ('sous vide', 0.0014854426619132501),\n",
       " ('chino bandido', 0.0014845054741139357),\n",
       " ('homer simpson', 0.0014767932489451476),\n",
       " ('preconceived notions', 0.0014710689767898006),\n",
       " ('harry potter', 0.001468819393042191),\n",
       " ('hon machi', 0.001467997651203758),\n",
       " ('jap chae', 0.0014670537191372476),\n",
       " ('neeps tatties', 0.0014614541468761417),\n",
       " ('auld dubliner', 0.001437607820586544),\n",
       " ('bells whistles', 0.0014371486969851814),\n",
       " ('beggars choosers', 0.0014368641875969882),\n",
       " ('fra diavolo', 0.0014367318677514281),\n",
       " ('bai thong', 0.0014325755216443475),\n",
       " ('cardiac arrest', 0.0014296304140633273),\n",
       " ('haagen dazs', 0.001424936386768448),\n",
       " ('bla bla', 0.0014202668780552252),\n",
       " ('highs lows', 0.0014202668780552252),\n",
       " ('scantily clad', 0.0014101134649439234),\n",
       " ('demi glace', 0.001409306952580966),\n",
       " ('val vista', 0.0014080741466892435)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:52:57.116677Z",
     "start_time": "2018-10-11T17:52:57.111959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_list_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:58:28.171544Z",
     "start_time": "2018-10-11T17:58:27.412391Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.9600000000000006\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('nlp__food_bigrams', lambda: final_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:49:43.814281Z",
     "start_time": "2018-10-11T17:49:43.740608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amuse bouche', 0.002449538880231712),\n",
       " ('panna cotta', 0.0024366664262408114),\n",
       " ('hong kong', 0.002368348656065774),\n",
       " ('dac biet', 0.0022203418262186697),\n",
       " ('kool aid', 0.00221921927720082),\n",
       " ('hush puppies', 0.002174993577926386),\n",
       " ('pei wei', 0.002159885646362734),\n",
       " ('rula bula', 0.0021597104711979357),\n",
       " ('wi fi', 0.0021574838154616743),\n",
       " ('http www', 0.0021246549830347475),\n",
       " ('joel robuchon', 0.002071413355549124),\n",
       " ('osso bucco', 0.0020663521320564973),\n",
       " ('pina colada', 0.0020560820705300357),\n",
       " ('bok choy', 0.002038502953195484),\n",
       " ('tres leches', 0.002027480482770849),\n",
       " ('valle luna', 0.0019444043001599273),\n",
       " ('croque madame', 0.0019429611874709279),\n",
       " ('prix fixe', 0.0019142608363247845),\n",
       " ('wal mart', 0.0019129014109983777),\n",
       " ('ami gabi', 0.0019120632510543846),\n",
       " ('hustle bustle', 0.0018992244982861065),\n",
       " ('krispy kreme', 0.0018890288611304914),\n",
       " ('coca cola', 0.0018856716025204975),\n",
       " ('beaten path', 0.0018605632783578686),\n",
       " ('kilt lifter', 0.0018466222309688651),\n",
       " ('huevos rancheros', 0.001834312652736427),\n",
       " ('pinot noir', 0.0018213800516045115),\n",
       " ('ropa vieja', 0.0018047336558411683),\n",
       " ('pet peeve', 0.0018009572635302403),\n",
       " ('wolfgang puck', 0.0017920889609268108),\n",
       " ('monte carlo', 0.001789524118348598),\n",
       " ('puerto rican', 0.0017892432065274063),\n",
       " ('moscow mule', 0.0017885641658015245),\n",
       " ('surf turf', 0.0017833077544022323),\n",
       " ('santa fe', 0.0017695761088450345),\n",
       " ('loco moco', 0.0017567611853666794),\n",
       " ('har gow', 0.0017399831633698052),\n",
       " ('pin kaow', 0.0017339833524513488),\n",
       " ('lotus siam', 0.001717849726427897),\n",
       " ('tex mex', 0.0017178197514905178),\n",
       " ('jean philippe', 0.001714241438525666),\n",
       " ('patatas bravas', 0.00169016762448993),\n",
       " ('halo halo', 0.0016778639194267911),\n",
       " ('lomo saltado', 0.0016694890913408476),\n",
       " ('arnold palmer', 0.001667079893562716),\n",
       " ('gulab jamun', 0.0016544992900081),\n",
       " ('carl jr', 0.0016461871895185368),\n",
       " ('toby keith', 0.0016461707482574668),\n",
       " ('bo hue', 0.001638125238163332),\n",
       " ('itty bitty', 0.001632981139993407),\n",
       " ('cabo wabo', 0.0016297190147085427),\n",
       " ('flip flops', 0.001623941984155059),\n",
       " ('peter piper', 0.0016052768468266302),\n",
       " ('scantily clad', 0.0016046477512846606),\n",
       " ('lactose intolerant', 0.0015995545790001486),\n",
       " ('kung pao', 0.001590157655521572),\n",
       " ('casey moore', 0.0015690401903504976),\n",
       " ('harry potter', 0.0015688791101121763),\n",
       " ('cochinita pibil', 0.0015624528251921845),\n",
       " ('insult injury', 0.0015532864204847888),\n",
       " ('sauvignon blanc', 0.0015509872149486806),\n",
       " ('conveyor belt', 0.0015502495353640702),\n",
       " ('ping pang', 0.0015394920610925266),\n",
       " ('julian serrano', 0.001538634587266404),\n",
       " ('monte cristo', 0.0015310701450909536),\n",
       " ('artery clogging', 0.0015263278532062368),\n",
       " ('kee mao', 0.0014999108367683668),\n",
       " ('kare kare', 0.0014977572271543964),\n",
       " ('pf changs', 0.0014920571004659404),\n",
       " ('saving grace', 0.0014908034308557603),\n",
       " ('knick knacks', 0.0014818893775711377),\n",
       " ('cave creek', 0.0014765664698304494),\n",
       " ('ho hum', 0.001469111818324945),\n",
       " ('pun intended', 0.001449768080195337),\n",
       " ('malai kofta', 0.0014485181986882715),\n",
       " ('michael mina', 0.001447693825904427),\n",
       " ('hubert keller', 0.001437092480759746),\n",
       " ('thit nuong', 0.0014326858930292763),\n",
       " ('cous cous', 0.0014325608817589647),\n",
       " ('tilted kilt', 0.0014281942579828658),\n",
       " ('bradley ogden', 0.0014281931665138343),\n",
       " ('baba ganoush', 0.0014249956429293416),\n",
       " ('osso buco', 0.0014205932617420841),\n",
       " ('beverly hills', 0.0014187597181318317),\n",
       " ('chick fil', 0.0014039831775969644),\n",
       " ('reina pepiada', 0.001377023396203968),\n",
       " ('aguas frescas', 0.0013768385413879967),\n",
       " ('vice versa', 0.0013761673444551926),\n",
       " ('johnny rockets', 0.0013733833956259581),\n",
       " ('lau lau', 0.0013691935741486088),\n",
       " ('neck woods', 0.0013645496585829894),\n",
       " ('caffe boa', 0.0013639695277383032),\n",
       " ('uuu uuu', 0.0013619341514462017),\n",
       " ('mario batali', 0.001351908837344992),\n",
       " ('butternut squash', 0.0013472830155233173),\n",
       " ('riff raff', 0.0013434844616901124),\n",
       " ('dietary restrictions', 0.0013336343903530717),\n",
       " ('tutti santi', 0.0013317332282009537),\n",
       " ('val vista', 0.0013298849531407626),\n",
       " ('khai hoan', 0.0013294860623596736)]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T20:10:18.723194Z",
     "start_time": "2018-10-10T20:10:18.717727Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_list[1:5] +final_list[6:102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T12:37:57.694591Z",
     "start_time": "2018-10-10T12:37:57.689740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'and': 6,\n",
       "         'document': 9,\n",
       "         'first': 7,\n",
       "         'is': 9,\n",
       "         'one': 6,\n",
       "         'second': 6,\n",
       "         'the': 9,\n",
       "         'third': 6,\n",
       "         'this': 9})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt + collections.Counter(dict.fromkeys(tt, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:36:54.520567Z",
     "start_time": "2018-10-11T17:36:54.516866Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document. Solve this interesting project.',\n",
    "     'This document is the second document and third one and another third one. Time-series analysis is interesting.',\n",
    "     'And this is the third one. Homework needs to be done. It is quite an interesting project.',\n",
    "     'Is this the first document? Which movie are we watching? Can we do Time-series homework now?',\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:36:55.080498Z",
     "start_time": "2018-10-11T17:36:55.077241Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=yelp_stopwords)\n",
    "vectorizer_bi = CountVectorizer(ngram_range=(2,2),stop_words=yelp_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:39:16.466524Z",
     "start_time": "2018-10-11T17:39:16.458659Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(stop_words=yelp_stopwords)\n",
    "x_tf=tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:39:38.271215Z",
     "start_time": "2018-10-11T17:39:38.265828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['analysis', 'document', 'homework', 'interesting', 'movie', 'project', 'series', 'solve', 'time', 'watching'] "
     ]
    }
   ],
   "source": [
    "print(tfidf.get_feature_names(),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:40:16.299613Z",
     "start_time": "2018-10-11T17:40:16.293006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4833548558354761,\n",
       " 1.334769978906423,\n",
       " 0.9951080723719671,\n",
       " 1.2142573236714682,\n",
       " 0.4838099584718287,\n",
       " 1.1187668445441918,\n",
       " 0.762523848576601,\n",
       " 0.6406554311067799,\n",
       " 0.762523848576601,\n",
       " 0.4838099584718287]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tf.sum(axis=0).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:36:55.571817Z",
     "start_time": "2018-10-11T17:36:55.563197Z"
    }
   },
   "outputs": [],
   "source": [
    "x=vectorizer.fit_transform(corpus)\n",
    "x_bi=vectorizer_bi.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:36:56.296360Z",
     "start_time": "2018-10-11T17:36:56.110590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 2, 3, 1, 2, 2, 1, 2, 1]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(axis=0).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:36:57.426450Z",
     "start_time": "2018-10-11T17:36:57.419054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bi.sum(axis=0).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:36:59.426450Z",
     "start_time": "2018-10-11T17:36:59.420976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['analysis',\n",
       " 'document',\n",
       " 'homework',\n",
       " 'interesting',\n",
       " 'movie',\n",
       " 'project',\n",
       " 'series',\n",
       " 'solve',\n",
       " 'time',\n",
       " 'watching']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:37:04.890283Z",
     "start_time": "2018-10-11T17:37:04.885526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['analysis interesting', 'document document', 'document movie', 'document solve', 'document time', 'homework interesting', 'interesting project', 'movie watching', 'series analysis', 'series homework', 'solve interesting', 'time series', 'watching time'] "
     ]
    }
   ],
   "source": [
    "print(vectorizer_bi.get_feature_names(),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T12:47:21.739442Z",
     "start_time": "2018-10-10T12:47:21.735645Z"
    }
   },
   "outputs": [],
   "source": [
    "tt_mono=collections.Counter(dict(list(zip(vectorizer.get_feature_names(),x.sum(axis=0).tolist()[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T12:47:49.383547Z",
     "start_time": "2018-10-10T12:47:49.378740Z"
    }
   },
   "outputs": [],
   "source": [
    "tt_bi=collections.Counter(dict(list(zip(vectorizer_bi.get_feature_names(),x_bi.sum(axis=0).tolist()[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T12:47:54.244798Z",
     "start_time": "2018-10-10T12:47:54.240018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'and': 1,\n",
       "         'document': 4,\n",
       "         'first': 2,\n",
       "         'is': 4,\n",
       "         'one': 1,\n",
       "         'second': 1,\n",
       "         'the': 4,\n",
       "         'third': 1,\n",
       "         'this': 4})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_mono"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T12:48:45.278906Z",
     "start_time": "2018-10-10T12:48:45.272856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'and this': 1,\n",
       "         'document is': 1,\n",
       "         'first document': 2,\n",
       "         'is the': 3,\n",
       "         'is this': 1,\n",
       "         'second document': 1,\n",
       "         'the first': 2,\n",
       "         'the second': 1,\n",
       "         'the third': 1,\n",
       "         'third one': 1,\n",
       "         'this document': 1,\n",
       "         'this is': 2,\n",
       "         'this the': 1})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_bi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T12:51:11.137912Z",
     "start_time": "2018-10-10T12:51:11.132831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'this']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'and this'.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T14:30:03.209363Z",
     "start_time": "2018-10-10T14:30:03.203248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and this 0.25\n",
      "document is 0.0625\n",
      "first document 0.25\n",
      "is the 0.1875\n",
      "is this 0.0625\n",
      "second document 0.25\n",
      "the first 0.25\n",
      "the second 0.25\n",
      "the third 0.25\n",
      "third one 1.0\n",
      "this document 0.0625\n",
      "this is 0.125\n",
      "this the 0.0625\n"
     ]
    }
   ],
   "source": [
    "for ab,val in tt_bi.items():\n",
    "    a,b=ab.split(\" \")\n",
    "    print (ab,val/(tt_mono[a]*tt_mono[b])*1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_prob = [biprobs[b]/(monoprobs[s[0]]*monoprobs[s[1]]) for b,s in zip(unique_biwords,bi_keys_split)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T14:24:08.034740Z",
     "start_time": "2018-10-10T14:24:08.031398Z"
    }
   },
   "outputs": [],
   "source": [
    "azip=([*tt_bi],list(map(lambda x: x.split(\" \"),[*tt_bi])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T14:27:03.728309Z",
     "start_time": "2018-10-10T14:27:03.721291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and this and this\n",
      "document is document is\n",
      "first document first document\n",
      "is the is the\n",
      "is this is this\n",
      "second document second document\n",
      "the first the first\n",
      "the second the second\n",
      "the third the third\n",
      "third one third one\n",
      "this document this document\n",
      "this is this is\n",
      "this the this the\n"
     ]
    }
   ],
   "source": [
    "for b,s in zip([*tt_bi],list(map(lambda x: x.split(\" \"),[*tt_bi]))):\n",
    "    print(b,s[0],s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T15:26:28.481462Z",
     "start_time": "2018-10-10T15:26:28.476527Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_dict=collections.Counter()\n",
    "for b,s in zip([*tt_bi],list(map(lambda x: x.split(\" \"),[*tt_bi]))):\n",
    "    temp_dict[b]= tt_bi[b]/(tt_mono[s[0]]*tt_mono[s[1]]*1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T15:22:50.632925Z",
     "start_time": "2018-10-10T15:22:50.629788Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_c=collections.Counter(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T15:26:43.207555Z",
     "start_time": "2018-10-10T15:26:43.202143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('third one', 1.0),\n",
       " ('and this', 0.25),\n",
       " ('first document', 0.25),\n",
       " ('second document', 0.25),\n",
       " ('the first', 0.25),\n",
       " ('the second', 0.25),\n",
       " ('the third', 0.25),\n",
       " ('is the', 0.1875)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_c.most_common(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T14:34:21.927440Z",
     "start_time": "2018-10-10T14:34:21.920512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1, 4, 0.25),\n",
       " (1, 4, 4, 0.0625),\n",
       " (2, 2, 4, 0.25),\n",
       " (3, 4, 4, 0.1875),\n",
       " (1, 4, 4, 0.0625),\n",
       " (1, 1, 4, 0.25),\n",
       " (2, 4, 2, 0.25),\n",
       " (1, 4, 1, 0.25),\n",
       " (1, 4, 1, 0.25),\n",
       " (1, 1, 1, 1.0),\n",
       " (1, 4, 4, 0.0625),\n",
       " (2, 4, 4, 0.125),\n",
       " (1, 4, 4, 0.0625)]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(tt_bi[b],tt_mono[s[0]],tt_mono[s[1]],\\\n",
    "         tt_bi[b]/(tt_mono[s[0]]*tt_mono[s[1]]*1.0)) \\\n",
    " for b,s in zip([*tt_bi],list(map(lambda x: x.split(\" \"),[*tt_bi])))]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T14:35:07.925281Z",
     "start_time": "2018-10-10T14:35:07.918904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.25,\n",
       " 0.0625,\n",
       " 0.25,\n",
       " 0.1875,\n",
       " 0.0625,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 0.25,\n",
       " 1.0,\n",
       " 0.0625,\n",
       " 0.125,\n",
       " 0.0625]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tt_bi[b]/(tt_mono[s[0]]*tt_mono[s[1]]*1.0) \\\n",
    " for b,s in zip([*tt_bi],list(map(lambda x: x.split(\" \"),[*tt_bi])))]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T14:35:03.641406Z",
     "start_time": "2018-10-10T14:35:03.633694Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.0625,\n",
       " 0.5,\n",
       " 0.1875,\n",
       " 0.0625,\n",
       " 1.0,\n",
       " 0.125,\n",
       " 0.0625,\n",
       " 0.0625,\n",
       " 1.0,\n",
       " 0.0625,\n",
       " 0.125,\n",
       " 0.0625]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tt_bi[b]/(tt_mono[s[0]]*tt_mono[s[0]]*1.0) \\\n",
    " for b,s in zip([*tt_bi],list(map(lambda x: x.split(\" \"),[*tt_bi])))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T14:19:17.840134Z",
     "start_time": "2018-10-10T14:19:17.833691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 1),\n",
       " ('document', 4),\n",
       " ('first', 2),\n",
       " ('is', 4),\n",
       " ('one', 1),\n",
       " ('second', 1),\n",
       " ('the', 4),\n",
       " ('third', 1),\n",
       " ('this', 4)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*tt_mono.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T14:42:45.732311Z",
     "start_time": "2018-10-10T14:42:45.725701Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and this', 1),\n",
       " ('document is', 1),\n",
       " ('first document', 2),\n",
       " ('is the', 3),\n",
       " ('is this', 1),\n",
       " ('second document', 1),\n",
       " ('the first', 2),\n",
       " ('the second', 1),\n",
       " ('the third', 1),\n",
       " ('third one', 1),\n",
       " ('this document', 1),\n",
       " ('this is', 2),\n",
       " ('this the', 1)]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*tt_bi.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T21:01:57.088509Z",
     "start_time": "2018-10-10T21:01:57.075767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hodge podge', 'himal chuli', 'hoity toity', 'roka akor', 'knick knacks', 'reina pepiada', 'cien agaves', 'baskin robbins', 'itty bitty', 'khai hoan', 'riff raff', 'grana padano', 'tutti santi', 'ropa vieja', 'gulab jamun', 'pel meni', 'ore ida', 'laan xang', 'dac biet', 'rula bula', 'hu tieu', 'innis gunn', 'bandeja paisa', 'tammie coe', 'chicha morada', 'alain ducasse', 'feng shui', 'pice rsistance', 'leaps bounds', 'dol sot', 'itsy bitsy', 'mille feuille', 'marche bacchus', 'uuu uuu', 'nooks crannies', 'celine dion', 'nanay gloria', 'doon varna', 'luc lac', 'krispy kreme', 'woonam jung', 'perrier jouet', 'deja vu', 'molecular gastronomy', 'puerto rican', 'vice versa', 'patatas bravas', 'sais quoi', 'cullen skink', 'lloyd wright', 'pura vida', 'lomo saltado', 'valle luna', 'nuoc mam', 'wal mart', 'dueling pianos', 'bradley ogden', 'barnes noble', 'avant garde', 'honky tonk', 'haricot vert', 'kao tod', 'irn bru', 'ak yelpcdn', 'porta alba', 'lis doon', 'khao soi', 'malai kofta', 'aguas frescas', 'loup mer', 'rx boiler', 'jos andrs', 'yadda yadda', 'mccormick schmick', 'yada yada', 'shiner bock', 'buen provecho', 'artery clogging', 'ritz carlton', 'womp womp', 'rinky dink', 'chino bandido', 'sous vide', 'preconceived notions', 'harry potter', 'hon machi', 'jap chae', 'neeps tatties', 'auld dubliner', 'fra diavolo', 'homer simpson', 'bai thong', 'cardiac arrest', 'haagen dazs', 'bells whistles', 'bla bla', 'highs lows', 'val vista', 'goi cuon', 'scantily clad']"
     ]
    }
   ],
   "source": [
    "print(final_list[:100],end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T14:42:15.898561Z",
     "start_time": "2018-10-10T14:42:15.891986Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'and this': 6,\n",
       "         'document is': 6,\n",
       "         'first document': 7,\n",
       "         'is the': 8,\n",
       "         'is this': 6,\n",
       "         'second document': 6,\n",
       "         'the first': 7,\n",
       "         'the second': 6,\n",
       "         'the third': 6,\n",
       "         'third one': 6,\n",
       "         'this document': 6,\n",
       "         'this is': 7,\n",
       "         'this the': 6})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_bi+collections.Counter(dict.fromkeys(tt_bi,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top100 = ['haricot vert'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.score('nlp__food_bigrams', lambda: top100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-10T16:03:30.107424Z",
     "start_time": "2018-10-10T16:03:29.946431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pel meni', 'f_5_unx wrafcxuakbzrdw', 'bandeja paisa', 'laan xang', 'roka akor', 'mille feuille', 'grana padano', 'innis gunn', 'chicha morada', 'dol sot', 'hodge podge', 'sais quoi', 'cullen skink', 'himal chuli', 'hoity toity', 'woonam jung', 'celine dion', 'perrier jouet', 'riff raff', 'luc lac', 'ore ida', 'baskin robbins', 'reina pepiada', 'rustler rooste', 'alain ducasse', 'ezzyujdouig4p gyb3pv_a', 'cien agaves', 'dueling pianos', 'deja vu', 'nanay gloria', 'homer simpson', 'khai hoan', 'hon machi'] is too short\n",
      "\n",
      "Failed validating 'minItems' in schema:\n",
      "    {'items': {'type': 'string'},\n",
      "     'maxItems': 100,\n",
      "     'minItems': 100,\n",
      "     'type': 'array'}\n",
      "\n",
      "On instance:\n",
      "    ['pel meni',\n",
      "     'f_5_unx wrafcxuakbzrdw',\n",
      "     'bandeja paisa',\n",
      "     'laan xang',\n",
      "     'roka akor',\n",
      "     'mille feuille',\n",
      "     'grana padano',\n",
      "     'innis gunn',\n",
      "     'chicha morada',\n",
      "     'dol sot',\n",
      "     'hodge podge',\n",
      "     'sais quoi',\n",
      "     'cullen skink',\n",
      "     'himal chuli',\n",
      "     'hoity toity',\n",
      "     'woonam jung',\n",
      "     'celine dion',\n",
      "     'perrier jouet',\n",
      "     'riff raff',\n",
      "     'luc lac',\n",
      "     'ore ida',\n",
      "     'baskin robbins',\n",
      "     'reina pepiada',\n",
      "     'rustler rooste',\n",
      "     'alain ducasse',\n",
      "     'ezzyujdouig4p gyb3pv_a',\n",
      "     'cien agaves',\n",
      "     'dueling pianos',\n",
      "     'deja vu',\n",
      "     'nanay gloria',\n",
      "     'homer simpson',\n",
      "     'khai hoan',\n",
      "     'hon machi']\n"
     ]
    }
   ],
   "source": [
    "grader.score('nlp__food_bigrams', lambda: final_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T17:55:19.469488Z",
     "start_time": "2018-10-11T17:55:19.456976Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-a8e3e1b00328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "final_results[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-11T18:04:14.252717Z",
     "start_time": "2018-10-11T18:04:13.688582Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score:  0.9600000000000006\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('nlp__food_bigrams', lambda: final_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2016 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nbclean": true,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "193px",
    "width": "254px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
